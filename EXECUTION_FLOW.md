# RAG FastAPI - Complete Execution Flow

## ğŸ“‹ Table of Contents
1. [Startup Sequence (docker-compose up)](#startup-sequence)
2. [User Ingest Flow](#user-ingest-flow)
3. [User Retrieve Flow](#user-retrieve-flow)
4. [User Chat Flow](#user-chat-flow)
5. [Health Check Flow](#health-check-flow)
6. [Complete Call Chain](#complete-call-chain)

---

## ğŸš€ Startup Sequence (docker-compose up)

### Step 1: Docker Compose Starts Services

```
docker-compose up
        â”‚
        â”œâ”€ Starts PostgreSQL container
        â”‚   â””â”€ Loads: init.sql (creates document_chunks table + pgvector extension)
        â”‚
        â””â”€ Starts FastAPI app container
            â””â”€ Runs: python -m uvicorn app.main:app
```

### Step 2: main.py Loads

**File:** `app/main.py` (Lines 1-40)

```python
# SEQUENCE OF EXECUTION:

1. from fastapi import FastAPI
   â””â”€ Imports FastAPI framework

2. from app.routes.ingest import router as ingest_router
   â””â”€ File: app/routes/ingest.py (Lines 1-70)
      â””â”€ Creates: ingest_service = IngestService()
         â””â”€ File: app/application/ingest_service.py (Lines 1-151)
            â””â”€ Calls: __init__(self)
               â”œâ”€ Line 47: self._embedder = embedder or get_embedder()
               â”‚  â””â”€ File: app/infrastructure/embedders/__init__.py
               â”‚     â””â”€ Calls: get_embedder()
               â”‚        â””â”€ File: app/infrastructure/embedders/minilm_embedder.py (Lines 1-80)
               â”‚           â””â”€ Calls: MiniLMEmbedder() (Singleton)
               â”‚              â””â”€ First time: Load 90MB model from HuggingFace
               â”‚              â””â”€ Logs: "Loading MiniLM embedding model (Singleton)..."
               â”‚              â””â”€ Wait 60+ seconds for model to download/load
               â”‚              â””â”€ Caches in memory for future use
               â”‚
               â”œâ”€ Line 48: self._vector_store = vector_store or PostgresVectorStore()
               â”‚  â””â”€ File: app/infrastructure/persistence/postgres_vector_store.py (Lines 1-183)
               â”‚     â””â”€ Calls: __init__(self)
               â”‚        â””â”€ Creates connection pool to PostgreSQL
               â”‚        â””â”€ Tries to connect to: localhost:5432 (DB host)
               â”‚        â””â”€ Waits for database to be ready
               â”‚
               â””â”€ Line 49: self._chunker = chunker or FixedSizeChunker(...)
                  â””â”€ File: app/infrastructure/chunkers/fixed_size_chunker.py
                     â””â”€ Calls: FixedSizeChunker(chunk_size=500, overlap=50)
                        â””â”€ Stores parameters, doesn't process anything yet

3. from app.routes.retrieve import router as retrieve_router
   â””â”€ File: app/routes/retrieve.py
      â””â”€ Creates: retrieval_service = RetrievalService()
         â””â”€ File: app/application/retrieval_service.py
            â””â”€ Same initialization as IngestService (shares embedder, vector_store)

4. from app.routes.chat import router as chat_router
   â””â”€ File: app/routes/chat.py
      â””â”€ Creates: chat_service = ChatService()
         â””â”€ File: app/application/chat_service.py
            â””â”€ Calls: __init__(self)
               â”œâ”€ Sets up embedder
               â”œâ”€ Sets up vector_store
               â”œâ”€ Sets up llm_provider
               â”‚  â””â”€ File: app/infrastructure/llm_providers/openrouter_adapter.py
               â”‚     â””â”€ Stores OpenRouter API key from .env
               â”‚
               â””â”€ Sets up prompt_builder
                  â””â”€ File: app/domain/builders/prompt_builder.py
                     â””â”€ Initializes: RAGPromptBuilder()

5. from app.routes.health import router as health_router
   â””â”€ File: app/routes/health.py
      â””â”€ No service initialization, just route definitions

6. app.include_router(ingest_router, prefix="/ingest", tags=["ingest"])
   â””â”€ Registers HTTP routes
   â””â”€ Routes become available at: POST /ingest/

7. app.include_router(retrieve_router, prefix="/retrieve", tags=["retrieve"])
   â””â”€ Routes available at: POST /retrieve/

8. app.include_router(chat_router, prefix="/chat", tags=["chat"])
   â””â”€ Routes available at: POST /chat/

9. app.include_router(health_router, prefix="/health", tags=["health"])
   â””â”€ Routes available at: GET /health/

10. logger.info("Starting RAG FastAPI application")
    â””â”€ File: app/core/logging.py
       â””â”€ Logs startup message

11. if __name__ == "__main__":
    â””â”€ Starts Uvicorn server
       â””â”€ Logs: "INFO:     Uvicorn running on http://0.0.0.0:8000"
```

---

## ğŸŸ¢ **APP IS NOW READY** âœ…

```
All services initialized
All routes registered
All dependencies loaded
Waiting for user requests...
```

---

## ğŸ“¤ User Ingest Flow (POST /ingest/)

### Scenario: User uploads "resume.pdf"

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ USER ACTION: Click "Upload" button in browser                  â•‘
â•‘ Sends: POST /ingest/ with file=resume.pdf, tag=resume         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        â”‚
        â–¼
HTTP Request hits FastAPI
        â”‚
        â–¼
app/routes/ingest.py - ingest_document() function (Line 21-51)
â”‚
â”œâ”€ Line 26: logger.info("Ingest request: resume.pdf, tag: resume")
â”‚   â””â”€ File: app/core/logging.py - logs the request
â”‚
â”œâ”€ Line 29: if not ingest_service.is_file_supported(file.filename)
â”‚   â””â”€ File: app/application/ingest_service.py - is_file_supported() (Line 53-54)
â”‚      â””â”€ Calls: DocumentLoaderFactory.is_supported("resume.pdf")
â”‚         â””â”€ File: app/infrastructure/document_loaders/loader_factory.py
â”‚            â””â”€ Checks if ".pdf" is in _loaders dictionary
â”‚            â””â”€ Returns: True
â”‚
â”œâ”€ Line 34: file_bytes = await file.read()
â”‚   â””â”€ Reads file from HTTP request
â”‚   â””â”€ Returns: bytes of PDF content
â”‚
â””â”€ Line 37-41: result = ingest_service.ingest(file_bytes, "resume.pdf", "resume")
   â”‚
   â””â”€ File: app/application/ingest_service.py - ingest() (Line 66-111)
      â”‚
      â”œâ”€ STEP 1: Load Document (Line 85)
      â”‚   â””â”€ loader = DocumentLoaderFactory.create_loader("resume.pdf")
      â”‚      â”‚
      â”‚      â””â”€ File: app/infrastructure/document_loaders/loader_factory.py
      â”‚         â”œâ”€ Extract extension: ".pdf"
      â”‚         â”œâ”€ Lookup _loaders[".pdf"]
      â”‚         â””â”€ Returns: PDFLoader() instance
      â”‚
      â”‚   â””â”€ pages = loader.load(file_bytes)
      â”‚      â”‚
      â”‚      â””â”€ File: app/infrastructure/document_loaders/pdf_loader.py
      â”‚         â”œâ”€ Uses: pdfplumber library to parse PDF
      â”‚         â”œâ”€ Extracts text from each page
      â”‚         â””â”€ Returns: [{"page": 1, "text": "..."}, {"page": 2, "text": "..."}, ...]
      â”‚
      â”œâ”€ STEP 2: Chunk Text (Line 89)
      â”‚   â””â”€ chunks = self._chunker.chunk(pages)
      â”‚      â”‚
      â”‚      â””â”€ File: app/infrastructure/chunkers/fixed_size_chunker.py - chunk() (Line 35-55)
      â”‚         â”œâ”€ Takes pages: [{"page": 1, "text": "..."}, ...]
      â”‚         â”œâ”€ Splits each page by chunk_size=500 characters
      â”‚         â”œâ”€ Adds overlap=50 characters between chunks
      â”‚         â””â”€ Returns: [
      â”‚            {"page": 1, "text": "...", "chunk_id": 0},
      â”‚            {"page": 1, "text": "...", "chunk_id": 1},
      â”‚            ...
      â”‚         ]
      â”‚
      â”œâ”€ STEP 3: Generate Embeddings (Line 93-96)
      â”‚   â””â”€ for chunk in chunks:
      â”‚      â””â”€ chunk["embedding"] = self._embedder.embed_text(chunk["text"])
      â”‚         â”‚
      â”‚         â””â”€ File: app/infrastructure/embedders/minilm_embedder.py - embed_text() (Line 65-72)
      â”‚            â”œâ”€ Takes: "Some text from chunk"
      â”‚            â”œâ”€ Uses: self._model.encode(text) (Singleton model)
      â”‚            â”œâ”€ Returns: [0.123, 0.456, 0.789, ...] (384 dimensions)
      â”‚            â”‚
      â”‚            â””â”€ Now chunk looks like:
      â”‚               {
      â”‚                 "page": 1,
      â”‚                 "text": "Some text from chunk",
      â”‚                 "chunk_id": 0,
      â”‚                 "embedding": [0.123, 0.456, 0.789, ...]
      â”‚               }
      â”‚
      â”œâ”€ STEP 4: Store in Vector Database (Line 100)
      â”‚   â””â”€ self._vector_store.add(chunks)
      â”‚      â”‚
      â”‚      â””â”€ File: app/infrastructure/persistence/postgres_vector_store.py - add() (Line 28-53)
      â”‚         â”œâ”€ Takes: chunks with embeddings
      â”‚         â”œâ”€ For each chunk, creates SQL INSERT:
      â”‚         â”‚   INSERT INTO document_chunks (content, embedding, tag, page_number, chunk_id)
      â”‚         â”‚   VALUES ('Some text', '[0.123, 0.456, ...]'::vector, 'resume', 1, 0)
      â”‚         â”œâ”€ Connects to PostgreSQL database
      â”‚         â”‚  â””â”€ File: app/db/models.py - get_connection() (Line 8-17)
      â”‚         â”‚     â”œâ”€ Reads: DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD from .env
      â”‚         â”‚     â””â”€ Calls: psycopg2.connect(...) to PostgreSQL
      â”‚         â”‚
      â”‚         â””â”€ Commits transaction to database
      â”‚
      â””â”€ STEP 5: Return Result (Line 103-111)
         â””â”€ Returns to app/routes/ingest.py
            â””â”€ Returns JSON: {"chunks_stored": 25, "filename": "resume.pdf", "tag": "resume"}
                â”‚
                â””â”€ Sent back to user's browser as HTTP response
                   â””â”€ User sees: "âœ“ 25 chunks stored successfully"
```

---

## ğŸ” User Retrieve Flow (POST /retrieve/)

### Scenario: User searches for "salary negotiation"

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ USER ACTION: Enter search query "salary negotiation"            â•‘
â•‘ Sends: POST /retrieve/ with query="salary negotiation"         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        â”‚
        â–¼
app/routes/retrieve.py - retrieve_documents() (Line 21-45)
â”‚
â”œâ”€ logger.info("Retrieve request: salary negotiation")
â”‚
â””â”€ File: app/application/retrieval_service.py - retrieve() (Line 53-93)
   â”‚
   â”œâ”€ STEP 1: Embed Query (Line 66)
   â”‚   â””â”€ query_embedding = self._embedder.embed_text("salary negotiation")
   â”‚      â”‚
   â”‚      â””â”€ File: app/infrastructure/embedders/minilm_embedder.py
   â”‚         â””â”€ Returns: [0.234, 0.567, 0.890, ...] (384 dimensions)
   â”‚
   â”œâ”€ STEP 2: Search Vector Database (Line 69)
   â”‚   â””â”€ results = self._vector_store.search(query_embedding, top_k=5, tag=tag)
   â”‚      â”‚
   â”‚      â””â”€ File: app/infrastructure/persistence/postgres_vector_store.py - search() (Line 83-120)
   â”‚         â”œâ”€ Connects to PostgreSQL
   â”‚         â”‚  â””â”€ File: app/db/models.py - get_connection()
   â”‚         â”‚
   â”‚         â”œâ”€ Runs SQL query:
   â”‚         â”‚   SELECT content, tag, page_number, chunk_id
   â”‚         â”‚   FROM document_chunks
   â”‚         â”‚   ORDER BY embedding <-> %s (vector similarity)
   â”‚         â”‚   LIMIT 5
   â”‚         â”‚
   â”‚         â”œâ”€ pgvector calculates similarity using: embedding <-> query_embedding
   â”‚         â”‚  â””â”€ pgvector extension (built into PostgreSQL container)
   â”‚         â”‚
   â”‚         â””â”€ Returns: Top 5 most similar chunks
   â”‚            [
   â”‚              {"content": "...", "page": 5, "chunk_id": 2},
   â”‚              {"content": "...", "page": 8, "chunk_id": 1},
   â”‚              ...
   â”‚            ]
   â”‚
   â””â”€ STEP 3: Return Results (Line 73-93)
      â””â”€ Returns to app/routes/retrieve.py
         â””â”€ Returns JSON: {"results": [...], "count": 5}
            â”‚
            â””â”€ Sent back to user's browser as HTTP response
```

---

## ğŸ’¬ User Chat Flow (POST /chat/)

### Scenario: User asks "What is the salary range?"

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ USER ACTION: Type message "What is the salary range?"           â•‘
â•‘ Sends: POST /chat/ with message="What is the salary range?"    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        â”‚
        â–¼
app/routes/chat.py - chat_endpoint() (Line 24-51)
â”‚
â”œâ”€ logger.info("Chat request: What is the salary range?")
â”‚
â””â”€ File: app/application/chat_service.py - chat() (Line 80-160)
   â”‚
   â”œâ”€ STEP 1: Infer Tag (Line 90)
   â”‚   â””â”€ inferred_tag = tag or infer_tag_from_text(message)
   â”‚      â”‚
   â”‚      â””â”€ File: app/services/tag_inference.py - infer_tag_from_text() (Line 1-14)
   â”‚         â”œâ”€ Checks if "salary" in message â†’ Detects "FINANCE" tag
   â”‚         â””â”€ Returns: "FINANCE"
   â”‚
   â”œâ”€ STEP 2: Retrieve Context (Line 93)
   â”‚   â””â”€ relevant_docs = self._retrieval_service.retrieve(message, tag=inferred_tag)
   â”‚      â”‚
   â”‚      â””â”€ File: app/application/retrieval_service.py - retrieve() (Line 53-93)
   â”‚         â”œâ”€ Embed message: "What is the salary range?"
   â”‚         â”‚  â””â”€ File: app/infrastructure/embedders/minilm_embedder.py
   â”‚         â”‚     â””â”€ Returns: [0.345, 0.678, 0.901, ...]
   â”‚         â”‚
   â”‚         â””â”€ Search vector database
   â”‚            â””â”€ File: app/infrastructure/persistence/postgres_vector_store.py - search()
   â”‚               â””â”€ Returns: Top 5 most similar documents
   â”‚                  â””â”€ Example: ["The salary range is $50k-$70k", "Bonuses are...", ...]
   â”‚
   â”œâ”€ STEP 3: Build Prompt (Line 96-99)
   â”‚   â””â”€ messages = (
   â”‚      â”‚    RAGPromptBuilder()
   â”‚      â”‚    .reset()
   â”‚      â”‚    .add_context(relevant_docs)
   â”‚      â”‚    .set_query(message)
   â”‚      â”‚    .build_messages()
   â”‚      â”‚)
   â”‚      â”‚
   â”‚      â””â”€ File: app/domain/builders/prompt_builder.py
   â”‚         â”œâ”€ RAGPromptBuilder (Line 1-150)
   â”‚         â”‚  â”œâ”€ reset(): Initialize empty components
   â”‚         â”‚  â”œâ”€ add_context(): Add retrieved documents
   â”‚         â”‚  â”œâ”€ set_query(): Add user's question
   â”‚         â”‚  â””â”€ build_messages(): Construct final prompt
   â”‚         â”‚
   â”‚         â””â”€ Returns: [
   â”‚            {
   â”‚              "role": "system",
   â”‚              "content": "You are a helpful assistant. Use the context to answer."
   â”‚            },
   â”‚            {
   â”‚              "role": "user",
   â”‚              "content": "Context:\nThe salary range is $50k-$70k\n\nQuestion: What is the salary range?"
   â”‚            }
   â”‚         ]
   â”‚
   â”œâ”€ STEP 4: Call LLM (Line 102)
   â”‚   â””â”€ response = self._llm_provider.chat(messages)
   â”‚      â”‚
   â”‚      â””â”€ File: app/infrastructure/llm_providers/openrouter_adapter.py - chat() (Line 85-120)
   â”‚         â”œâ”€ Takes messages array
   â”‚         â”œâ”€ Calls: requests.post("https://openrouter.ai/api/v1/chat/completions", ...)
   â”‚         â”‚  â”œâ”€ Sends: {
   â”‚         â”‚  â”‚    "model": "mistralai/mistral-7b-instruct",
   â”‚         â”‚  â”‚    "messages": [...]
   â”‚         â”‚  â”‚  }
   â”‚         â”‚  â””â”€ Authenticates with: Authorization: Bearer {API_KEY}
   â”‚         â”‚
   â”‚         â””â”€ Waits for OpenRouter API response (3-10 seconds)
   â”‚            â””â”€ Returns: "The salary range is $50k-$70k based on the documents provided."
   â”‚
   â”œâ”€ STEP 5: Tag Document (Line 105-116)
   â”‚   â””â”€ Saves chat message to database with tag
   â”‚      â””â”€ File: app/infrastructure/persistence/postgres_vector_store.py
   â”‚         â””â”€ Stores for future context/history
   â”‚
   â””â”€ STEP 6: Return Response (Line 117-127)
      â””â”€ Returns to app/routes/chat.py
         â””â”€ Returns JSON: {
            "response": "The salary range is $50k-$70k based on the documents provided.",
            "sources": ["resume.pdf (page 5, chunk 2)", ...]
         }
            â”‚
            â””â”€ Sent back to user's browser
               â””â”€ User sees answer + document sources
```

---

## ğŸ¥ Health Check Flow (GET /health/)

### Scenario: Monitor service health

```
GET /health/
        â”‚
        â–¼
app/routes/health.py - health_check() (Line 20-50)
â”‚
â”œâ”€ Check PostgreSQL connection
â”‚   â””â”€ File: app/db/models.py - get_connection()
â”‚      â””â”€ Tries to connect to PostgreSQL
â”‚         â””â”€ Returns: âœ“ Connected or âœ— Failed
â”‚
â”œâ”€ Check MiniLM model
â”‚   â””â”€ File: app/infrastructure/embedders/minilm_embedder.py
â”‚      â””â”€ Checks if model is loaded
â”‚         â””â”€ Returns: âœ“ Loaded or âœ— Failed
â”‚
â””â”€ Returns JSON: {
   "status": "healthy",
   "database": "connected",
   "embedder": "loaded"
}
```

---

## ğŸ”— Complete Call Chain Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DOCKER COMPOSE UP                                               â”‚
â”‚ â”œâ”€ PostgreSQL Container (init.sql)                              â”‚
â”‚ â””â”€ FastAPI Container (app/main.py)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APP INITIALIZATION (ONE TIME)                                   â”‚
â”‚ â”œâ”€ app/main.py loads                                            â”‚
â”‚ â”œâ”€ app/routes/ingest.py â†’ IngestService()                       â”‚
â”‚ â”‚   â””â”€ app/application/ingest_service.py â†’ __init__()           â”‚
â”‚ â”‚       â”œâ”€ MiniLMEmbedder() (Singleton - load model)            â”‚
â”‚ â”‚       â”œâ”€ PostgresVectorStore()                                â”‚
â”‚ â”‚       â””â”€ FixedSizeChunker()                                   â”‚
â”‚ â”œâ”€ app/routes/retrieve.py â†’ RetrievalService()                  â”‚
â”‚ â”‚   â””â”€ app/application/retrieval_service.py â†’ __init__()        â”‚
â”‚ â”œâ”€ app/routes/chat.py â†’ ChatService()                           â”‚
â”‚ â”‚   â””â”€ app/application/chat_service.py â†’ __init__()             â”‚
â”‚ â”‚       â”œâ”€ RAGPromptBuilder()                                   â”‚
â”‚ â”‚       â””â”€ OpenRouterAdapter()                                  â”‚
â”‚ â””â”€ app/routes/health.py (no initialization)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SERVER READY - WAITING FOR USER REQUESTS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                      â”‚                      â”‚
        â–¼                     â–¼                      â–¼                      â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ INGEST  â”‚            â”‚ RETRIEVE â”‚          â”‚  CHAT  â”‚            â”‚ HEALTH â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                    â”‚
        â–¼                      â–¼                    â–¼
   routes/ingest.py      routes/retrieve.py   routes/chat.py
        â”‚                      â”‚                    â”‚
        â–¼                      â–¼                    â–¼
   IngestService         RetrievalService      ChatService
        â”‚                      â”‚                    â”‚
        â”œâ”€ Factory             â”œâ”€ Embedder         â”œâ”€ Tag Inference
        â”‚  (PDFLoader)         â”‚                   â”‚
        â”œâ”€ Chunker             â””â”€ Vector Store     â”œâ”€ Retrieval Service
        â”‚                                          â”‚
        â”œâ”€ Embedder                                â”œâ”€ Prompt Builder
        â”‚                                          â”‚
        â””â”€ Vector Store                            â”œâ”€ LLM Adapter
                                                   â”‚
                                                   â””â”€ Vector Store
```

---

## ğŸ“Š File Dependency Map

```
app/
â”œâ”€â”€ main.py (ENTRY POINT - runs first)
â”‚   â”œâ”€â”€ routes/ingest.py
â”‚   â”‚   â””â”€â”€ application/ingest_service.py
â”‚   â”‚       â”œâ”€â”€ infrastructure/embedders/minilm_embedder.py (Singleton)
â”‚   â”‚       â”œâ”€â”€ infrastructure/persistence/postgres_vector_store.py
â”‚   â”‚       â”‚   â””â”€â”€ db/models.py (get_connection)
â”‚   â”‚       â”œâ”€â”€ infrastructure/chunkers/fixed_size_chunker.py (Strategy)
â”‚   â”‚       â””â”€â”€ infrastructure/document_loaders/loader_factory.py (Factory)
â”‚   â”‚           â””â”€â”€ infrastructure/document_loaders/pdf_loader.py
â”‚   â”‚
â”‚   â”œâ”€â”€ routes/retrieve.py
â”‚   â”‚   â””â”€â”€ application/retrieval_service.py
â”‚   â”‚       â”œâ”€â”€ infrastructure/embedders/minilm_embedder.py (reused)
â”‚   â”‚       â””â”€â”€ infrastructure/persistence/postgres_vector_store.py (reused)
â”‚   â”‚
â”‚   â”œâ”€â”€ routes/chat.py
â”‚   â”‚   â””â”€â”€ application/chat_service.py
â”‚   â”‚       â”œâ”€â”€ application/retrieval_service.py (reused)
â”‚   â”‚       â”œâ”€â”€ domain/builders/prompt_builder.py (Builder)
â”‚   â”‚       â”œâ”€â”€ infrastructure/llm_providers/openrouter_adapter.py (Adapter)
â”‚   â”‚       â”œâ”€â”€ services/tag_inference.py (Tag detection)
â”‚   â”‚       â””â”€â”€ infrastructure/persistence/postgres_vector_store.py (reused)
â”‚   â”‚
â”‚   â””â”€â”€ routes/health.py
â”‚       â””â”€â”€ db/models.py (get_connection)
â”‚
â”œâ”€â”€ core/logging.py (used by all files)
â””â”€â”€ domain/interfaces/ (used by all infrastructure files)
```

---

## â±ï¸ Timeline View

```
TIME 00:00 - docker-compose up
TIME 00:05 - PostgreSQL starts, init.sql runs
TIME 00:10 - FastAPI app starts, main.py loads
TIME 00:20 - IngestService() created, MiniLM model loading starts...
TIME 01:20 - MiniLM model loaded (90 seconds to download/load)
TIME 01:21 - All services initialized, server ready

TIME 01:25 - USER UPLOADS resume.pdf
TIME 01:26 - PDF loaded (1 second)
TIME 01:27 - Text chunked (0.5 seconds)
TIME 01:28 - Embeddings generated (10 seconds for 25 chunks)
TIME 01:39 - Chunks stored to PostgreSQL (0.5 seconds)
TIME 01:40 - User sees "25 chunks stored" âœ“

TIME 02:00 - USER SEARCHES for "salary negotiation"
TIME 02:01 - Query embedded (0.5 seconds)
TIME 02:02 - Vector search in PostgreSQL (0.5 seconds)
TIME 02:03 - Results returned âœ“

TIME 02:30 - USER ASKS "What is the salary range?"
TIME 02:31 - Tag inferred: FINANCE (instant)
TIME 02:32 - Relevant docs retrieved (1 second)
TIME 02:33 - Prompt built (instant)
TIME 02:34 - OpenRouter API called (5-10 seconds)
TIME 02:45 - LLM response received âœ“
```

---

## ğŸ”„ Data Flow Example: Complete Chat Request

```
USER INPUT: "What is the salary range?"
        â”‚
        â”œâ”€ [route: app/routes/chat.py]
        â”‚
        â”œâ”€ [service: app/application/chat_service.py]
        â”‚   â”œâ”€ Step 1: Tag Inference
        â”‚   â”‚   â””â”€ app/services/tag_inference.py
        â”‚   â”‚      â””â”€ Output: tag="FINANCE"
        â”‚   â”‚
        â”‚   â”œâ”€ Step 2: Retrieve Relevant Documents
        â”‚   â”‚   â””â”€ app/application/retrieval_service.py
        â”‚   â”‚       â”œâ”€ Embed query
        â”‚   â”‚       â”‚  â””â”€ app/infrastructure/embedders/minilm_embedder.py
        â”‚   â”‚       â”‚     â””â”€ Output: [0.234, 0.567, ...]
        â”‚   â”‚       â”‚
        â”‚   â”‚       â””â”€ Search database
        â”‚   â”‚           â””â”€ app/infrastructure/persistence/postgres_vector_store.py
        â”‚   â”‚               â””â”€ app/db/models.py (connection)
        â”‚   â”‚                  â””â”€ Output: ["The salary is $50k-70k", ...]
        â”‚   â”‚
        â”‚   â”œâ”€ Step 3: Build Prompt
        â”‚   â”‚   â””â”€ app/domain/builders/prompt_builder.py
        â”‚   â”‚      â””â”€ RAGPromptBuilder
        â”‚   â”‚         â””â”€ Output: [
        â”‚   â”‚              {"role": "system", "content": "..."},
        â”‚   â”‚              {"role": "user", "content": "..."}
        â”‚   â”‚            ]
        â”‚   â”‚
        â”‚   â””â”€ Step 4: Call LLM
        â”‚       â””â”€ app/infrastructure/llm_providers/openrouter_adapter.py
        â”‚          â””â”€ OpenRouter API
        â”‚             â””â”€ Output: "The salary range is $50k-$70k based on..."
        â”‚
        â””â”€ [route returns response to user]
           â””â”€ Browser displays answer âœ“
```

---

## ğŸ¯ Summary of File Execution Order

### On Startup (main.py):
1. `app/main.py` â­ STARTS HERE
2. `app/routes/ingest.py` - creates IngestService
3. `app/application/ingest_service.py` - initializes services
4. `app/infrastructure/embedders/minilm_embedder.py` - loads model
5. `app/infrastructure/persistence/postgres_vector_store.py` - connects to DB
6. `app/infrastructure/chunkers/fixed_size_chunker.py` - initializes
7. Similar for retrieve and chat routes
8. Server ready

### On Ingest Request:
1. `app/routes/ingest.py` - receives HTTP request
2. `app/application/ingest_service.py` - orchestrates
3. `app/infrastructure/document_loaders/loader_factory.py` - selects loader
4. `app/infrastructure/document_loaders/pdf_loader.py` - loads PDF
5. `app/infrastructure/chunkers/fixed_size_chunker.py` - chunks text
6. `app/infrastructure/embedders/minilm_embedder.py` - embeds chunks
7. `app/infrastructure/persistence/postgres_vector_store.py` - stores chunks
8. `app/db/models.py` - manages DB connection

### On Retrieve Request:
1. `app/routes/retrieve.py` - receives request
2. `app/application/retrieval_service.py` - orchestrates
3. `app/infrastructure/embedders/minilm_embedder.py` - embeds query
4. `app/infrastructure/persistence/postgres_vector_store.py` - searches
5. `app/db/models.py` - manages DB connection

### On Chat Request:
1. `app/routes/chat.py` - receives request
2. `app/application/chat_service.py` - orchestrates
3. `app/services/tag_inference.py` - infers tag
4. `app/application/retrieval_service.py` - gets context
5. `app/domain/builders/prompt_builder.py` - builds prompt
6. `app/infrastructure/llm_providers/openrouter_adapter.py` - calls LLM
7. Response returned to user

---

## ğŸ—‚ï¸ Folder Structure with Execution Context

```
rag_fastapi/
â”œâ”€â”€ Dockerfile ........................... Docker image definition
â”œâ”€â”€ docker-compose.yml ................... Orchestrates PostgreSQL + App
â”œâ”€â”€ init.sql ............................ Creates DB schema (runs once)
â”œâ”€â”€ requirements.txt ..................... Python dependencies
â”‚
â””â”€â”€ app/
    â”œâ”€â”€ main.py â­ ...................... ENTRY POINT - loads everything
    â”‚
    â”œâ”€â”€ routes/ (HTTP Controllers - Thin Layer)
    â”‚   â”œâ”€â”€ ingest.py ................... POST /ingest/ endpoint
    â”‚   â”œâ”€â”€ retrieve.py ................. POST /retrieve/ endpoint
    â”‚   â”œâ”€â”€ chat.py ..................... POST /chat/ endpoint
    â”‚   â””â”€â”€ health.py ................... GET /health/ endpoint
    â”‚
    â”œâ”€â”€ application/ (Service Layer - Facade)
    â”‚   â”œâ”€â”€ ingest_service.py ........... Orchestrates document ingestion
    â”‚   â”œâ”€â”€ retrieval_service.py ........ Orchestrates document retrieval
    â”‚   â””â”€â”€ chat_service.py ............. Orchestrates RAG chat
    â”‚
    â”œâ”€â”€ infrastructure/ (Implementations - Concrete)
    â”‚   â”œâ”€â”€ embedders/
    â”‚   â”‚   â””â”€â”€ minilm_embedder.py ...... Generates embeddings (Singleton)
    â”‚   â”œâ”€â”€ persistence/
    â”‚   â”‚   â””â”€â”€ postgres_vector_store.py  Stores vectors (Repository)
    â”‚   â”œâ”€â”€ document_loaders/
    â”‚   â”‚   â”œâ”€â”€ pdf_loader.py ........... Loads PDF files
    â”‚   â”‚   â””â”€â”€ loader_factory.py ....... Creates loaders (Factory)
    â”‚   â”œâ”€â”€ chunkers/
    â”‚   â”‚   â””â”€â”€ fixed_size_chunker.py ... Chunks text (Strategy)
    â”‚   â””â”€â”€ llm_providers/
    â”‚       â””â”€â”€ openrouter_adapter.py ... Calls OpenRouter API (Adapter)
    â”‚
    â”œâ”€â”€ domain/ (Abstractions - Contracts)
    â”‚   â”œâ”€â”€ interfaces/
    â”‚   â”‚   â”œâ”€â”€ embedder.py ............. IEmbedder interface
    â”‚   â”‚   â”œâ”€â”€ vector_store.py ......... IVectorStore interface
    â”‚   â”‚   â”œâ”€â”€ document_loader.py ...... IDocumentLoader interface
    â”‚   â”‚   â”œâ”€â”€ chunker.py .............. IChunker interface
    â”‚   â”‚   â””â”€â”€ llm_provider.py ......... ILLMProvider interface
    â”‚   â””â”€â”€ builders/
    â”‚       â””â”€â”€ prompt_builder.py ....... Builds prompts (Builder)
    â”‚
    â”œâ”€â”€ db/
    â”‚   â””â”€â”€ models.py ................... get_connection() function
    â”‚
    â”œâ”€â”€ services/
    â”‚   â””â”€â”€ tag_inference.py ............ Auto-detects document tags
    â”‚
    â””â”€â”€ core/
        â””â”€â”€ logging.py .................. Logging utility
```

---

This is your complete mental map! Every file, every folder, every execution step. ğŸ¯
